# -*- coding: utf-8 -*-
"""Sistemaragdefipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1snuQZjiT03-F0QjV0E_BgH59w0hmflHv
"""

# -*- coding: utf-8 -*-
"""Sistemaragdefipynb

Generado automaticamente por Colab

el archivo original se encuentra en:
    https://colab.research.google.com/drive/1snuQZjiT03-F0QjV0E_BgH59w0hmflHv
"""

!pip install -q langchain
!pip install -q torch
!pip install -q transformers
!pip install -q sentence-transformers
!pip install -q datasets
!pip install -q faiss-cpu
!pip install -q pypdf
!pip install tqdm
!pip install -U langchain-community

from langchain.document_loaders import HuggingFaceDatasetLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter #divisiones d texto
from langchain.embeddings import HuggingFaceEmbeddings # Permite generar representaciones numéricas (embeddings) de textos usando modelos de Hugging Face
from langchain.vectorstores import FAISS # buscador de similitud
from transformers import AutoTokenizer, AutoModelForQuestionAnswering # AutoModelForQuestionAnswering carga un modelo preentrenado para tareas de preguntas y respuestas
from transformers import AutoTokenizer, pipeline # Permite crear pipelines (flujos automáticos) con modelos de Hugging Face
from langchain import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch # las otras librerias pueden estarlo llamando
from langchain import PromptTemplate, LLMChain

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
    AutoModelForCausalLM
    )

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") #cuda permite correr los algoitmos para que se demoren menos
print(device) # Muestra en pantalla qué tipo de dispositivo se usará (cuda o cpu)

!mkdir -p ./pneumonia_docs
!cp "/content/drive/MyDrive/repaso/manual_neumo_nm-180-193.pdf" ./pneumonia_docs/IMSS_Guia_Neumonia.pdf
!ls ./pneumonia_docs

import os  #  para manejar rutas y archivos del sistema operativo
path_docs = "./pneumonia_docs" # Define la ruta donde se guardaron los documentos PDF

os.listdir(path_docs) # Lista los archivos dentro de la carpeta especificada y los muestra por pantalla

# transformacion de los documentos (leerlos y prepararlos para el modelo)
from tqdm.notebook import tqdm
from langchain.document_loaders import PyPDFLoader #cargar y leer archivos PDF.
from langchain.text_splitter import RecursiveCharacterTextSplitter #divisiones d texto


# Parámetros de división
chunk_size = 700 # longitud máxima de cada fragmento de texto en caracteres.
chunk_overlap = 100 # chunk coge los limites de contexto

# Crear lista de fragmentos
all_docs = []
text_splitter = RecursiveCharacterTextSplitter(  #divide el texto en fragmentos de tamaño determinado.
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
    ) # llena la lista con los fragmentos de texto

# Leer y dividir cada PDF
for name in tqdm(os.listdir(path_docs)):#recorrer todos los archivos pdf
    if name.endswith(".pdf"): #filtro adicicional para tomar solo archivos que finalicen en .pdf
        path_tmp = os.path.join(path_docs, name) #construcción de la ruta completa
        loader = PyPDFLoader(path_tmp) #carga el archivo pdf y lealo
        text = loader.load() #extracción del texto del pdf
        chunks_of_text = text_splitter.split_documents(text) # aplicación de división por fragmentos
        all_docs.extend(chunks_of_text) #agrega los fragmentos a la lista

print(f" Se generaron {len(all_docs)} fragmentos de texto.")

"""**modelos de embedding**"""

from pprint import pprint

def print_lines(text, max_ch=50): #max_ch establece el máximo de caracteres por línea
    current_line = ""  # Acumula las palabras de la línea actual
    for word in text.split():  # Recorre el texto palabra por palabra
        if len(current_line) + len(word) + 1 > max_ch:  # Si agregar la siguiente palabra supera el límite de caracteres, imprime la línea actual
            print(current_line)
            current_line = word  # Inicia una nueva línea con la palabra actual
        else:
            current_line += " " + word  # Si no se supera el límite, agrega la palabra a la línea con un espacio
    if current_line:
        print(current_line)

chunk = all_docs[0]
print_lines(chunk.page_content)
pprint(chunk.metadata)

def get_embeddings_model(model_path=None):
  # Modelos de embeddings
  model1 = "sentence-transformers/all-MiniLM-l6-v2"
  model2 = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  model3 = "intfloat/multilingual-e5-base"

  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  print(f"device: {device}")

  model_kwargs = {'device': device} # Define en qué dispositivo se ejecutará
  encode_kwargs = {'normalize_embeddings': False}  # Indica que no se normalicen los embeddings generados


  embeddings1 = HuggingFaceEmbeddings(
      model_name=model1,
      model_kwargs=model_kwargs,
      encode_kwargs=encode_kwargs
  )

  embeddings2 = HuggingFaceEmbeddings(
      model_name=model2,
      model_kwargs=model_kwargs,
      encode_kwargs=encode_kwargs
  )

  embeddings3 = HuggingFaceEmbeddings(
      model_name=model3,
      model_kwargs=model_kwargs,
      encode_kwargs=encode_kwargs
  )

  print(" Modelos cargados correctamente:")
  print("-", model1)
  print("-", model2)
  print("-", model3)

  return embeddings1, embeddings2, embeddings3

embeddings1, embeddings2, embeddings3 = get_embeddings_model()

text = "La neumonía es una infección que afecta los pulmones." # Texto de prueba que se convertirá en un vector numérico (embedding)

modelos = [embeddings1, embeddings2, embeddings3] # Lista con los tres modelos de embeddings cargados anteriormente

contador = 1

# Recorre cada modelo de la lista y genera su embedding del texto
for modelo in modelos:
  # Convierte el texto en un vector (embedding) usando el modelo actual
    resultado = modelo.embed_query(text)
    print("El modelo", contador, "tiene", len(resultado), "dimensiones")
    contador = contador + 1  # Aumenta el contador para pasar al siguiente model

# Crea tres bases vectoriales (vectorstores) usando FAISS,
# una por cada modelo de embeddings para comparar su desempeño

db1 = FAISS.from_documents(all_docs, embeddings1)
db2 = FAISS.from_documents(all_docs, embeddings2)
db3 = FAISS.from_documents(all_docs, embeddings3)

pregunta = "¿sintomas de la neumonia?"

# Busca los 5 fragmentos más similares en cada base vectorial (una por modelo)
search1 = db1.similarity_search(pregunta, k=5)
search2 = db2.similarity_search(pregunta, k=5)
search3 = db3.similarity_search(pregunta, k=5)

# Muestra el fragmento más relevante encontrado por cada modelo
print(" Resultado con el modelo 1:")
print_lines(search1[0].page_content)

print("\n Resultado con el modelo 2:")
print_lines(search2[0].page_content)

print("\n Resultado con el modelo 3:")
print_lines(search3[0].page_content)

import re #libreria para limpiar texto

# Función que obtiene el contexto más relevante para una pregunta
def get_context(db, question, top_k=2):
    searchDocs = db.similarity_search(question, k=top_k)
    texto = "\n".join([x.page_content for x in searchDocs])
    texto_limpio = re.sub(r"\t+", " ", texto)
    return texto_limpio[:3000]


pregunta = "¿sintomas de la neumonia?"

# Obtiene los contextos más relevantes de cada modelo
contexto1 = get_context(db1, pregunta)
contexto2 = get_context(db2, pregunta)
contexto3 = get_context(db3, pregunta)


print(" Contexto del modelo 1:\n")
print(contexto1)

print("\n Contexto del modelo 2:\n")
print(contexto2)

print("\n Contexto del modelo 3:\n")
print(contexto3)

"""**LLM: modelos the question-answering**

"""

import torch
from transformers import AutoModelForQuestionAnswering
from transformers import TFAutoModelForQuestionAnswering

"""*con mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es*"""

question = "¿Qué es la neumonía?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿cual es el tratamiento para nac ambulatoria?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿duración del tratamiento antibiótico para pacientes que no requieren ingreso?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac que precisa ingreso en uci, del grupo III?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

"""*modelo deepset/minilm-uncased-squad2*"""

question = "¿Qué es la neumonía?"
context = get_context(db3, question, top_k=9)


model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac ambulatorio?"
context = get_context(db3, question, top_k=9)


model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿duración del tratamiento antibiótico para pacientes que no requieren ingreso?"
context = get_context(db3, question, top_k=9)

model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac que precisa ingreso en uci, del grupo III?"
context = get_context(db3, question, top_k=9)

model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

"""* modelo distilbert-base-multilingual-cased*"""

question = "¿Qué es la neumonía?"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿cual es el tratamiento para nac ambulatoria?"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿duración del tratamiento antibiótico para pacientes que no requieren ingreso?"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac que precisa ingreso en uci, del grupo III??"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

"""**pregunta ingresada por el usuario:**"""

question = input("\ningrese una pregunta:").strip()
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

