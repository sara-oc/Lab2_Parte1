# -*- coding: utf-8 -*-
"""Sistemaragdefipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1snuQZjiT03-F0QjV0E_BgH59w0hmflHv
"""

!pip install -q langchain
!pip install -q torch
!pip install -q transformers
!pip install -q sentence-transformers
!pip install -q datasets
!pip install -q faiss-cpu
!pip install -q pypdf
!pip install tqdm
!pip install -U langchain-community

from langchain.document_loaders import HuggingFaceDatasetLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter #divisiones d texto
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS # buscador de similitud
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from transformers import AutoTokenizer, pipeline
from langchain import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch # las otras librerias pueden estarlo llamando
from langchain import PromptTemplate, LLMChain

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
    AutoModelForCausalLM
    )

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") #cuda permite correr los algoitmos para que se demoren menos
print(device) # Muestra en pantalla qué tipo de dispositivo se usará (cuda o cpu)

!mkdir -p ./pneumonia_docs
!cp "/content/drive/MyDrive/repaso/manual_neumo_nm-180-193.pdf" ./pneumonia_docs/IMSS_Guia_Neumonia.pdf
!ls ./pneumonia_docs

import os  #  para manejar rutas y archivos del sistema operativo
path_docs = "./pneumonia_docs" # Define la ruta donde se guardaron los documentos PDF

os.listdir(path_docs) # Lista los archivos dentro de la carpeta especificada y los muestra por pantalla

# transformacion de los documentos (leerlos y prepararlos para el modelo)
from tqdm.notebook import tqdm
from langchain.document_loaders import PyPDFLoader #cargar y leer archivos PDF.
from langchain.text_splitter import RecursiveCharacterTextSplitter #divisiones d texto


# Parámetros de división
chunk_size = 700 # longitud máxima de cada fragmento de texto en caracteres.
chunk_overlap = 100 # chunk coge los limites de contexto

# Crear lista de fragmentos
all_docs = []
text_splitter = RecursiveCharacterTextSplitter(  #divide el texto en fragmentos de tamaño determinado.
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
    ) # llena la lista con los fragmentos de texto

# Leer y dividir cada PDF
for name in tqdm(os.listdir(path_docs)):#recorrer todos los archivos pdf
    if name.endswith(".pdf"): #filtro adicicional para tomar solo archivos que finalicen en .pdf
        path_tmp = os.path.join(path_docs, name) #construcción de la ruta completa
        loader = PyPDFLoader(path_tmp) #carga el archivo pdf y lealo
        text = loader.load() #extracción del texto del pdf
        chunks_of_text = text_splitter.split_documents(text) # aplicación de división por fragmentos
        all_docs.extend(chunks_of_text) #agrega los fragmentos a la lista

print(f" Se generaron {len(all_docs)} fragmentos de texto.")

"""**modelos de embedding**"""

from pprint import pprint

def print_lines(text, max_ch=50): #max_ch establece el máximo de caracteres por línea
    current_line = ""  # Acumula las palabras de la línea actual
    for word in text.split():  # Recorre el texto palabra por palabra
        if len(current_line) + len(word) + 1 > max_ch:  # Si agregar la siguiente palabra supera el límite de caracteres, imprime la línea actual
            print(current_line)
            current_line = word  # Inicia una nueva línea con la palabra actual
        else:
            current_line += " " + word  # Si no se supera el límite, agrega la palabra a la línea con un espacio
    if current_line:
        print(current_line)

chunk = all_docs[0]
print_lines(chunk.page_content)
pprint(chunk.metadata)

def get_embeddings_model(model_path=None):
  # Modelos de embeddings
  model1 = "sentence-transformers/all-MiniLM-l6-v2"
  model2 = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  model3 = "intfloat/multilingual-e5-base"

  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  print(f"device: {device}")

  model_kwargs = {'device': device} # Define en qué dispositivo se ejecutará
  encode_kwargs = {'normalize_embeddings': False}  # Indica que no se normalicen los embeddings generados


  embeddings1 = HuggingFaceEmbeddings(
      model_name=model1,
      model_kwargs=model_kwargs,
      encode_kwargs=encode_kwargs
  )

  embeddings2 = HuggingFaceEmbeddings(
      model_name=model2,
      model_kwargs=model_kwargs,
      encode_kwargs=encode_kwargs
  )

  embeddings3 = HuggingFaceEmbeddings(
      model_name=model3,
      model_kwargs=model_kwargs,
      encode_kwargs=encode_kwargs
  )

  print(" Modelos cargados correctamente:")
  print("-", model1)
  print("-", model2)
  print("-", model3)

  return embeddings1, embeddings2, embeddings3

"""**Modelos seleccionados**

Se eligieron tres modelos de embeddings: sentence-transformers/all-MiniLM-l6-v2, sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 y intfloat/multilingual-e5-base, porque permiten evaluar diferentes combinaciones de rendimiento, soporte para el idioma español y precisión semántica. El primero es un modelo base rápido y liviano, ideal para pruebas iniciales; el segundo añade comprensión multilingüe, lo que mejora su desempeño en textos en español; y el tercero, aunque más pesado, ofrece una representación semántica más precisa y fue el que mejor respondió en las pruebas con preguntas clínicas sobre neumonía.

Se desecharon otros modelos como distiluse-base-multilingual-cased-v2, all-mpnet-base-v2, sentence-t5-base y multi-qa-MiniLM-L6-cos-v1, ya que o bien están optimizados solo para inglés, no ofrecen mejoras significativas en comprensión en español, o requieren mayores recursos computacionales, lo que los hace poco prácticos para un sistema de recuperación ligera. Además, se descartaron versiones más grandes de la familia E5 y MiniLM (como intfloat/multilingual-e5-large o all-MiniLM-L12-v2) porque su tamaño aumentaba considerablemente el tiempo de respuesta y el consumo de memoria sin un incremento proporcional en la precisión. Por tanto, se mantuvieron los tres modelos seleccionados por su equilibrio entre desempeño, compatibilidad con español y eficiencia computacional.
"""

embeddings1, embeddings2, embeddings3 = get_embeddings_model()

text = "La neumonía es una infección que afecta los pulmones." # Texto de prueba que se convertirá en un vector numérico (embedding)

modelos = [embeddings1, embeddings2, embeddings3] # Lista con los tres modelos de embeddings cargados anteriormente

contador = 1

# Recorre cada modelo de la lista y genera su embedding del texto
for modelo in modelos:
  # Convierte el texto en un vector (embedding) usando el modelo actual
    resultado = modelo.embed_query(text)
    print("El modelo", contador, "tiene", len(resultado), "dimensiones")
    contador = contador + 1  # Aumenta el contador para pasar al siguiente model

# Crea tres bases vectoriales (vectorstores) usando FAISS,
# una por cada modelo de embeddings para comparar su desempeño

db1 = FAISS.from_documents(all_docs, embeddings1)
db2 = FAISS.from_documents(all_docs, embeddings2)
db3 = FAISS.from_documents(all_docs, embeddings3)

pregunta = "¿sintomas de la neumonia?"

# Busca los 5 fragmentos más similares en cada base vectorial (una por modelo)
search1 = db1.similarity_search(pregunta, k=5)
search2 = db2.similarity_search(pregunta, k=5)
search3 = db3.similarity_search(pregunta, k=5)

# Muestra el fragmento más relevante encontrado por cada modelo
print(" Resultado con el modelo 1:")
print_lines(search1[0].page_content)

print("\n Resultado con el modelo 2:")
print_lines(search2[0].page_content)

print("\n Resultado con el modelo 3:")
print_lines(search3[0].page_content)

import re #libreria para limpiar texto

# Función que obtiene el contexto más relevante para una pregunta
def get_context(db, question, top_k=2):
    searchDocs = db.similarity_search(question, k=top_k)
    texto = "\n".join([x.page_content for x in searchDocs])
    texto_limpio = re.sub(r"\t+", " ", texto)
    return texto_limpio[:3000]


pregunta = "¿sintomas de la neumonia?"

# Obtiene los contextos más relevantes de cada modelo
contexto1 = get_context(db1, pregunta)
contexto2 = get_context(db2, pregunta)
contexto3 = get_context(db3, pregunta)


print(" Contexto del modelo 1:\n")
print(contexto1)

print("\n Contexto del modelo 2:\n")
print(contexto2)

print("\n Contexto del modelo 3:\n")
print(contexto3)

"""Al evaluar los tres modelos de embeddings frente a la pregunta ¿síntomas de la neumonía?, se observó que el modelo 3 (intfloat/multilingual-e5-base) fue el que ofreció el mejor resultado, ya que recuperó un fragmento del texto donde se describen directamente las manifestaciones clínicas de la enfermedad: malestar general, fiebre, escalofríos, tos, expectoración y dolor torácico, entre otros. Este modelo mostró una mayor precisión semántica, identificando la sección del documento más relevante en función del significado de la pregunta.

Por el contrario, el modelo 1 (all-MiniLM-l6-v2), al estar entrenado principalmente en inglés y con menor capacidad semántica, recuperó un párrafo relacionado con la falta de respuesta terapéutica, sin conexión con los síntomas. El modelo 2 (paraphrase-multilingual-MiniLM-L12-v2), aunque multilingüe y compatible con español, priorizó información sobre tratamientos y clasificación de neumonías, lo que indica que comprendió el tema general, pero no la intención específica de la pregunta.

En conclusión, el modelo intfloat/multilingual-e5-base destacó por su mayor capacidad para relacionar términos médicos y contexto clínico, proporcionando la respuesta más completa y relevante sobre los síntomas de la neumonía, razón por la cual fue considerado el modelo más adecuado para este tipo de consultas médicas.

**LLM: modelos the question-answering**
"""

import torch
from transformers import AutoModelForQuestionAnswering
from transformers import TFAutoModelForQuestionAnswering

"""Se seleccionaron tres modelos de lenguaje con el objetivo de evaluar el desempeño en tareas de pregunta–respuesta aplicadas a textos clínicos en español, considerando la influencia del idioma y la eficiencia computacional.

El modelo mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es se eligió por estar entrenado específicamente en español, lo que lo convertía en una referencia ideal para analizar el comportamiento de un modelo monolingüe en un contexto médico técnico. El modelo deepset/minilm-uncased-squad2, aunque entrenado en inglés, se incluyó por su alta eficiencia y generalización semántica, además de ser un modelo compacto (MiniLM) ampliamente probado en tareas de comprensión lectora. Su inclusión buscaba verificar si un modelo en inglés podía mantener un buen rendimiento con documentos en español, lo cual finalmente se comprobó. Por último, distilbert-base-multilingual-cased fue seleccionado por su capacidad multilingüe y bajo costo computacional, lo que permite ejecutar el sistema en entornos con recursos limitados y comparar su desempeño frente a modelos más grandes.

En conjunto, estos tres modelos permitieron contrastar precisión, idioma y eficiencia, cubriendo desde un modelo entrenado exclusivamente en español hasta uno multilingüe y otro en inglés de arquitectura liviana. Esta selección equilibrada garantizó una evaluación completa del rendimiento del sistema RAG en distintas condiciones lingüísticas y de procesamiento.

con mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es
"""

question = "¿Qué es la neumonía?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿cual es el tratamiento para nac ambulatoria?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿duración del tratamiento antibiótico para pacientes que no requieren ingreso?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac que precisa ingreso en uci, del grupo III?"
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

"""**modelo deepset/minilm-uncased-squad2**"""

question = "¿Qué es la neumonía?"
context = get_context(db3, question, top_k=9)


model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac ambulatorio?"
context = get_context(db3, question, top_k=9)


model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

question = "¿duración del tratamiento antibiótico para pacientes que no requieren ingreso?"
context = get_context(db3, question, top_k=9)

model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac que precisa ingreso en uci, del grupo III?"
context = get_context(db3, question, top_k=9)

model_ckpt = "deepset/minilm-uncased-squad2"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

"""se puede notar que este modelo trbaja mejor y logra resporder a esa parte rara

** modelo distilbert-base-multilingual-cased**
"""

question = "¿Qué es la neumonía?"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿cual es el tratamiento para nac ambulatoria?"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿duración del tratamiento antibiótico para pacientes que no requieren ingreso?"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

question = "¿tratamiento para nac que precisa ingreso en uci, del grupo III??"
context = get_context(db3, question, top_k=9)

model_ckpt = "distilbert-base-multilingual-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)

pipe(question=question, context=context, topk=3)

"""Al evaluar los tres modelos de lenguaje (LLM), se observó que el modelo mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es ofreció los mejores resultados, mostrando respuestas coherentes, completas y directamente relacionadas con las preguntas clínicas, gracias a su entrenamiento específico en español y su afinación con el conjunto SQuAD2-es(Esto obliga al modelo no solo a encontrar la respuesta correcta cuando existe, sino también a reconocer cuándo no hay información suficiente en el texto). En cambio, el segundo modelo, deepset/minilm-uncased-squad2, aunque eficiente y rápido, tiende a confundirse con el idioma, generando respuestas parciales o inexactas al procesar textos médicos en español debido a su entrenamiento exclusivo en inglés. Finalmente, el modelo distilbert-base-multilingual-cased presentó un desempeño notablemente inferior: sus respuestas fueron vagas, incompletas y, en ocasiones, incoherentes, lo que se explica por su menor capacidad de comprensión contextual y la simplificación inherente a su arquitectura liviana. En síntesis, el primer modelo fue seleccionado por su equilibrio óptimo entre precisión semántica, dominio del idioma y fiabilidad en la extracción de información médica.

**pregunta ingresada por el usuario:**
"""

question = input("\ningrese una pregunta:").strip()
context = get_context(db3, question, top_k=3)


model_ckpt = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

inputs = tokenizer(question, context, return_tensors="pt")
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)


pipe(question=question, context=context, topk=3)

